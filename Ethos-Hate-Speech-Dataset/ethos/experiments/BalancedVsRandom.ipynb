{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IlqliX9gdv8D",
    "outputId": "f98cb9d3-dad6-4e42-b5fe-99adc6278c4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/johnmollas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/johnmollas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "#####################################################################\n",
    "#                      Balanced vs Ranodm                           #\n",
    "#####################################################################\n",
    "#                                                                   #\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from utilities.preprocess import Preproccesor\n",
    "from utilities.attention_layer import Attention\n",
    "from utilities.helping_functions import create_embedding_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, SpatialDropout1D, Bidirectional, Dense, \\\n",
    "    LSTM, Conv1D, Dropout, concatenate\n",
    "from keras import Input, Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading our Binary Data and the binary external dataset D1: Davidson, Thomas, et al. \"Automated hate speech detection and the problem of offensive language.\" Proceedings of the International AAAI Conference on Web and Social Media. Vol. 11. No. 1. 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "9dCSMfw-eUJ6",
    "outputId": "617655d5-5c50-4f04-b5c1-cd086435c528"
   },
   "outputs": [],
   "source": [
    "X, y = Preproccesor.load_data(True)\n",
    "X_tweets, y_tweets = Preproccesor.load_external_data(True)\n",
    "class_names = ['noHateSpeech', 'hateSpeech']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We will create two subsets (of the same size) of our binary dataset, one completely random, and one maintaining balance between classes. We will train then an SVM model and we will evaluate on the rest of our initial dataset. We will do the same for the external dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready in 10\n",
      "Ready in 9\n",
      "Ready in 8\n",
      "Ready in 7\n",
      "Ready in 6\n",
      "Ready in 5\n",
      "Ready in 4\n",
      "Ready in 3\n",
      "Ready in 2\n",
      "Ready in 1\n",
      "Accuracy on Train:\n",
      "  Random 0.97966376499681 0.005378974684487675\n",
      "  Balanced 0.9808823529411764 0.0038306280165492997\n",
      "Accuracy on Valid:\n",
      "  Random 0.6314528804791895 0.03931654165911837\n",
      "  Balanced 0.6798714281580398 0.02167657082961586\n",
      "Accuracy on Tweets:\n",
      "  Random 0.5062 0.010989085494252916\n",
      "  Balanced 0.43610000000000004 0.12387207110563707\n",
      "F1 on Train:\n",
      "  Random 0.9807072184575079 0.005128155255837031\n",
      "  Balanced 0.9808812133263045 0.0038307833930770595\n",
      "F1 on Valid:\n",
      "  Random 0.6418969967121514 0.04890115884441745\n",
      "  Balanced 0.6906148775114815 0.022944219034009755\n",
      "F1 on Tweets:\n",
      "  Random 0.3615152907320149 0.010478921171478238\n",
      "  Balanced 0.3721146662326628 0.08249712158863223\n",
      "F1 on Tweets NonHate:\n",
      "  Random 0.6653220800604149 0.010088513290533238\n",
      "  Balanced 0.5448483303218514 0.16317434416833318\n",
      "F1 on Tweets Hate:\n",
      "  Random 0.05770850140361501 0.018680498275611763\n",
      "  Balanced 0.19938100214347432 0.0334334703983608\n"
     ]
    }
   ],
   "source": [
    "acc_train = []\n",
    "acc_valid = []\n",
    "acc_tweets = []\n",
    "f1_train = []\n",
    "f1_valid = []\n",
    "f1_tweets = []\n",
    "f1_tweets_hate = []\n",
    "f1_tweets_nohate = []\n",
    "for iteration in range(10):\n",
    "    print(\"Ready in\", 10-iteration)\n",
    "    data = shuffle(X, y, random_state=777 + iteration)\n",
    "    X_temp = data[0]\n",
    "    y_temp = data[1]\n",
    "    c_0 = int((len(y_temp)-sum(y_temp))*0.875)\n",
    "    c_1 = int((sum(y_temp))*0.875)\n",
    "\n",
    "    x_val = X_temp[c_0+c_1:]  # We will test on this 12.5% of data\n",
    "    y_val = y_temp[c_0+c_1:]\n",
    "\n",
    "    # The rest available data are the 87.5% of the original\n",
    "    x_rest = X_temp[:c_0+c_1]\n",
    "    y_rest = y_temp[:c_0+c_1]\n",
    "\n",
    "    c_0 = int((len(y_temp)-sum(y_temp))*0.75)  # We will select\n",
    "    c_1 = int((sum(y_temp))*0.75)\n",
    "\n",
    "    x_random = x_rest[:c_0+c_1]  # 75% of them randomly\n",
    "    y_random = y_rest[:c_0+c_1]\n",
    "\n",
    "    c_0 = int((len(y_temp)-sum(y_temp))*0.875)\n",
    "    c_1 = int((sum(y_temp))*0.875)\n",
    "\n",
    "    x_75 = []\n",
    "    y_75 = []\n",
    "\n",
    "    c_min = min(c_0, c_1)\n",
    "    if c_min > int(len(y_temp)*0.75/2):\n",
    "        c_min = int(len(y_temp)*0.75/2)\n",
    "    c_0 = c_min\n",
    "    c_1 = c_min\n",
    "    for i in range(len(y_temp)):  # 75% of them maintaining class balance\n",
    "        if y_temp[i] == 0 and c_0 > 0:\n",
    "            x_75.append(X_temp[i])\n",
    "            y_75.append(y_temp[i])\n",
    "            c_0 = c_0 - 1\n",
    "        elif y_temp[i] == 1 and c_1 > 0:\n",
    "            x_75.append(X_temp[i])\n",
    "            y_75.append(y_temp[i])\n",
    "            c_1 = c_1 - 1\n",
    "    training_data = {'Random:': [x_random, y_random], 'Balanced:': [x_75, y_75]}\n",
    "    for k, v in training_data.items():\n",
    "        x_train = v[0]\n",
    "        y_train = v[1]\n",
    "\n",
    "        vec = TfidfVectorizer(analyzer='word', max_features=5000,\n",
    "                              ngram_range=(1, 2), stop_words='english')\n",
    "        vec.fit(X_tweets)\n",
    "\n",
    "        x_train = vec.transform(x_train)\n",
    "        x_valid = vec.transform(x_val)\n",
    "        x_tweets = vec.transform(X_tweets)\n",
    "\n",
    "        svm = SVC(kernel='rbf')\n",
    "        svm.fit(x_train, y_train)\n",
    "\n",
    "        y_predict = svm.predict(x_train)\n",
    "        # print(\"Train\",k)\n",
    "        acc_train.append([k, balanced_accuracy_score(y_train, y_predict)])\n",
    "        f1_train.append([k, f1_score(y_train, y_predict, average='weighted')])\n",
    "\n",
    "        y_predict = svm.predict(x_valid)\n",
    "        # print(\"Valid\",k)\n",
    "        acc_valid.append([k, balanced_accuracy_score(y_val, y_predict)])\n",
    "        f1_valid.append([k, f1_score(y_val, y_predict, average='weighted')])\n",
    "\n",
    "        #y_predict = svm.predict(x_tweets)\n",
    "        y_predict = svm.predict(x_tweets)\n",
    "\n",
    "        # print(\"Tweets\",k)\n",
    "        acc_tweets.append([k, balanced_accuracy_score(y_tweets, y_predict)])\n",
    "        f1_tweets.append([k, f1_score(y_tweets, y_predict, average='weighted')])\n",
    "        f1_tweets_nohate.append(\n",
    "            [k, f1_score(y_tweets, y_predict, average=None)[0]])\n",
    "        f1_tweets_hate.append([k, f1_score(y_tweets, y_predict, average=None)[1]])\n",
    "nnames = [\"Accuracy on Train:\", \"Accuracy on Valid:\", \"Accuracy on Tweets:\", \"F1 on Train:\",\n",
    "          \"F1 on Valid:\", \"F1 on Tweets:\", \"F1 on Tweets NonHate:\", \"F1 on Tweets Hate:\"]\n",
    "cc = 0\n",
    "print(\"Printing results:\")\n",
    "for i in [acc_train, acc_valid, acc_tweets, f1_train, f1_valid, f1_tweets, f1_tweets_nohate, f1_tweets_hate]:\n",
    "    r_i = []\n",
    "    r_b = []\n",
    "    for j in i:\n",
    "        if j[0] == 'Random:':\n",
    "            r_i.append(j[1])\n",
    "        else:\n",
    "            r_b.append(j[1])\n",
    "    r_i = np.array(r_i)\n",
    "    r_b = np.array(r_b)\n",
    "    print(nnames[cc])\n",
    "    print('  Random', r_i.mean(), r_i.std())\n",
    "    print('  Balanced', r_b.mean(), r_b.std())\n",
    "    cc = cc + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The resulrs propose that better performance has the balanced subset. On both the rest of ETHOS Data, and the external data (on the hate class, which is the minority class as well in this dataset). "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "CNN+Class.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}