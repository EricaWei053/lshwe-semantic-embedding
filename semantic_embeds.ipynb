{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semantic.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZtyh1PFyKQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e6ebf7b-4b6a-4763-adde-94ec60221ed7"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5EEFxg_zwN2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3dfed53-785b-417e-fb91-8745b0b4ddff"
      },
      "source": [
        "# install and import nltk package \n",
        "!sudo pip install -U nltk\n",
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4anLmBmFbACA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa936407-8335-40bc-880b-1c77d9dd8267"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "import keras\n",
        "import numpy as np\n",
        "import sklearn as sk"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW5y8Fm5d08p"
      },
      "source": [
        "data = pd.read_csv(\"Ethos_Dataset_Binary.csv\",delimiter=\";\")\n",
        "label = data[\"isHate\"]\n",
        "comments = data[\"comment\"]\n",
        "target = label.to_numpy()\n",
        "for i in range(len(target)):\n",
        "  if target[i] >= .5:\n",
        "    target[i] = 1\n",
        "  else:\n",
        "    target[i] = 0"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlGp5Mt9z9dB"
      },
      "source": [
        "Preprocessing dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fptu2XiUgfLV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3a9c074-9fbd-4032-bb04-536652935532"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "import string\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "for i in range(len(comments)):\n",
        "  comments[i] = word_tokenize(comments[i])\n",
        "  comments[i]=[w.lower() for w in comments[i]]\n",
        "  stripped = [w.translate(table) for w in comments[i]]\n",
        "  comments[i] = [w for w in stripped if w.isalpha()]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj-AcotFeGWH"
      },
      "source": [
        "def dict(x):\n",
        "    dic = []\n",
        "    counts = []\n",
        "    for item in x:\n",
        "        for word in item:\n",
        "            if word in dic:\n",
        "                index = dic.index(word)\n",
        "                counts[index] += 1\n",
        "            else:\n",
        "                dic.append(word)\n",
        "                counts.append(1)\n",
        "    return dic, counts\n",
        "\n",
        "dic, counts = dict(comments)\n",
        "wordcount = len(dic)\n",
        "window_size = 10\n",
        "rarewords_threshold = 3"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGWAucVlxVR3"
      },
      "source": [
        "build co-occurrence matrix "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZNkXFrVZ7Pg"
      },
      "source": [
        "cooc_matr = np.zeros([wordcount, wordcount])\n",
        "for item in comments:\n",
        "    for i in range(len(item)):\n",
        "        cenword = item[i]\n",
        "        cen_id = dic.index(cenword)\n",
        "        for j in range(i - window_size, i + window_size + 1):\n",
        "            if j in range(len(item)):\n",
        "                val = window_size + 1 - abs(i - j)\n",
        "                coword = item[j]\n",
        "                co_id = dic.index(coword)\n",
        "                cooc_matr[cen_id][co_id] += val\n",
        "\n",
        "for i in range(wordcount):\n",
        "    cooc_matr[i] = cooc_matr[i] / cooc_matr[i][i] * 1.0"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwrcnnbgxf4P"
      },
      "source": [
        "Getting rarewords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JCNSqWhaWKF"
      },
      "source": [
        "rarewords = []\n",
        "for i in range(wordcount):\n",
        "    if counts[i] < rarewords_threshold:\n",
        "        rarewords.append(i)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia0f1gOw9rqn"
      },
      "source": [
        "#Local sensitivity hashing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_Dn74bExj_w"
      },
      "source": [
        "Use local sensitivity hashing on co-occurrence matrix "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbRklUVjn__6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5f0a565-cd4a-4964-db2f-5d10c255d8e1"
      },
      "source": [
        "pip install lshashpy3"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lshashpy3\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/5e/746d7c54f883b1c0b216771d023b8e91e86b41eb9240b97baee8a849962f/lshashpy3-0.0.8.tar.gz\n",
            "Building wheels for collected packages: lshashpy3\n",
            "  Building wheel for lshashpy3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lshashpy3: filename=lshashpy3-0.0.8-cp37-none-any.whl size=8879 sha256=e6e164de7a81d8391fa59889adf238b6758dcf567cfbbe2279f5c65b0a4d27b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/81/29/a48985e27d56ddea4e601fda82b9be9962d20801b3cfa82c2d\n",
            "Successfully built lshashpy3\n",
            "Installing collected packages: lshashpy3\n",
            "Successfully installed lshashpy3-0.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLHVK-ZtrHTS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "001ee360-8c60-4384-c9b9-9fcb3f3a1c30"
      },
      "source": [
        "from lshashpy3 import LSHash\n",
        "hashsize = 200\n",
        "friends = 5\n",
        "#lsh = LSHash(window_size, wordcount)\n",
        "lsh = LSHash(hashsize, wordcount)\n",
        "for i in range(len(cooc_matr)):\n",
        "    lsh.index(cooc_matr[i])\n",
        "\n",
        "friendslist = []\n",
        "sim_matr = []\n",
        "for i in range(len(rarewords)):\n",
        "    result = lsh.query(cooc_matr[rarewords[i]], num_results=friends, distance_func='euclidean')\n",
        "    length = len(result)\n",
        "    fri = []\n",
        "    sim = []\n",
        "    for j in range(length):\n",
        "        fri.append(str(np.array(result[j][0])))\n",
        "        sim.append(float(result[j][1]))\n",
        "    if length < friends:\n",
        "        count = friends - length\n",
        "        for i in range(count):\n",
        "            fri.append(str(8))\n",
        "            sim.append(-1)\n",
        "    friendslist.append(fri)\n",
        "    sim_matr.append(sim)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMbDiI1KxvMT"
      },
      "source": [
        "Start trainning a embedding to approximate LSH "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM5wA2z9xpvo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e09c5fd3-cd98-40b5-dfe6-a4fdcc65964b"
      },
      "source": [
        "dim = 200\n",
        "learning_rate = 0.01\n",
        "training_epoch = 50\n",
        "X = tf.placeholder(\"float\", [wordcount, wordcount])\n",
        "sim_true = tf.placeholder(\"float\", [len(rarewords), friends])\n",
        "sim_pred = tf.placeholder(\"float\", [len(rarewords), friends])\n",
        "n_hidden_1 = 200\n",
        "n_hidden_2 = dim\n",
        "\n",
        "weights = {\n",
        "    'encoder_h1': tf.Variable(tf.random_normal([wordcount, n_hidden_1])),\n",
        "    'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
        "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
        "    'decoder_h2': tf.Variable(tf.random_normal([n_hidden_1, wordcount])),\n",
        "}\n",
        "biases = {\n",
        "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
        "    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "    'decoder_b2': tf.Variable(tf.random_normal([wordcount])),\n",
        "}\n",
        "\n",
        "def encoder(x):\n",
        "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
        "                                   biases['encoder_b1']))\n",
        "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n",
        "                                   biases['encoder_b2']))\n",
        "    return layer_2\n",
        "\n",
        "def decoder(x):\n",
        "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
        "                                   biases['decoder_b1']))\n",
        "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
        "                                   biases['decoder_b2']))\n",
        "    return layer_2\n",
        "\n",
        "cooc_true = X\n",
        "encoder_op = encoder(X)\n",
        "cooc_pred = decoder(encoder_op)\n",
        "cost = tf.reduce_mean(tf.pow(cooc_true - cooc_pred, 2))\n",
        "cost_min = cost + tf.reduce_mean(tf.pow(sim_true - sim_pred, 2))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost_min)\n",
        "\n",
        "if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
        "    init = tf.initialize_all_variables()\n",
        "else:\n",
        "    init = tf.global_variables_initializer()\n",
        "with tf.Session() as sess:\n",
        "    sess.graph.finalize()\n",
        "    sess.run(init)\n",
        "    for epoch in range(training_epoch):\n",
        "        word_list = sess.run(encoder_op, feed_dict={X: cooc_matr})\n",
        "\n",
        "        lsh = LSHash(hashsize, dim)\n",
        "        for item in word_list:\n",
        "            lsh.index(item)\n",
        "\n",
        "        cos = []\n",
        "        for i in range(len(rarewords)):\n",
        "            result = lsh.query(word_list[rarewords[i]], num_results=friends, distance_func='euclidean')\n",
        "            cosin = -1 * np.ones(friends)\n",
        "            for item in result:\n",
        "                id = str(np.array(item[0]))\n",
        "                for j in range(friends):\n",
        "                    if id == friendslist[i][j]:\n",
        "                        cosin[j] = float(item[1])\n",
        "            cos.append(cosin)\n",
        "\n",
        "        cost_fin, _ = sess.run([cost_min, optimizer], feed_dict={sim_pred: cos, sim_true: sim_matr, X: cooc_matr})\n",
        "        print(\"epoch:\",epoch, \",cost=\", \"{:.9f}\".format(cost_fin))\n",
        "    print(\"Optimization Finished!\")\n",
        "    encoder_result = sess.run(encoder_op, feed_dict={X: cooc_matr})"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 ,cost= 0.660462379\n",
            "epoch: 1 ,cost= 0.612342834\n",
            "epoch: 2 ,cost= 0.569455504\n",
            "epoch: 3 ,cost= 0.530999422\n",
            "epoch: 4 ,cost= 0.495752871\n",
            "epoch: 5 ,cost= 0.463900566\n",
            "epoch: 6 ,cost= 0.434885740\n",
            "epoch: 7 ,cost= 0.407879949\n",
            "epoch: 8 ,cost= 0.383017242\n",
            "epoch: 9 ,cost= 0.361087978\n",
            "epoch: 10 ,cost= 0.342323333\n",
            "epoch: 11 ,cost= 0.326621652\n",
            "epoch: 12 ,cost= 0.313843638\n",
            "epoch: 13 ,cost= 0.303415418\n",
            "epoch: 14 ,cost= 0.295010030\n",
            "epoch: 15 ,cost= 0.288330436\n",
            "epoch: 16 ,cost= 0.282978833\n",
            "epoch: 17 ,cost= 0.278550059\n",
            "epoch: 18 ,cost= 0.274766207\n",
            "epoch: 19 ,cost= 0.271492362\n",
            "epoch: 20 ,cost= 0.268699348\n",
            "epoch: 21 ,cost= 0.266308367\n",
            "epoch: 22 ,cost= 0.264064759\n",
            "epoch: 23 ,cost= 0.262019396\n",
            "epoch: 24 ,cost= 0.260514557\n",
            "epoch: 25 ,cost= 0.259384871\n",
            "epoch: 26 ,cost= 0.258308351\n",
            "epoch: 27 ,cost= 0.257273674\n",
            "epoch: 28 ,cost= 0.256363213\n",
            "epoch: 29 ,cost= 0.255582005\n",
            "epoch: 30 ,cost= 0.254899859\n",
            "epoch: 31 ,cost= 0.254309386\n",
            "epoch: 32 ,cost= 0.253789842\n",
            "epoch: 33 ,cost= 0.253352225\n",
            "epoch: 34 ,cost= 0.252999038\n",
            "epoch: 35 ,cost= 0.252633154\n",
            "epoch: 36 ,cost= 0.252194345\n",
            "epoch: 37 ,cost= 0.251758307\n",
            "epoch: 38 ,cost= 0.251321971\n",
            "epoch: 39 ,cost= 0.250812382\n",
            "epoch: 40 ,cost= 0.250238687\n",
            "epoch: 41 ,cost= 0.249623924\n",
            "epoch: 42 ,cost= 0.248962194\n",
            "epoch: 43 ,cost= 0.248359695\n",
            "epoch: 44 ,cost= 0.247759417\n",
            "epoch: 45 ,cost= 0.247170851\n",
            "epoch: 46 ,cost= 0.246730417\n",
            "epoch: 47 ,cost= 0.246433824\n",
            "epoch: 48 ,cost= 0.246171534\n",
            "epoch: 49 ,cost= 0.245889604\n",
            "Optimization Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhpsh6IWyA3z"
      },
      "source": [
        "load Word2Vec "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNEYfOFHYPyX"
      },
      "source": [
        "W2v = Word2Vec(comments)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o8P4gdUyEm8"
      },
      "source": [
        "Begin word similarity task, using wordsim355 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAR5awEueume"
      },
      "source": [
        "from scipy import spatial\n",
        "wordsim= pd.read_csv(\"wordsim.txt\",delimiter = \"\\t\")\n",
        "word1 = wordsim[\"tiger\"]\n",
        "word2 = wordsim[\"cat\"]\n",
        "similarity = wordsim[\"7.35\"]\n",
        "word11 = []\n",
        "word22 = []\n",
        "w2v1 = []\n",
        "w2v2 = []\n",
        "simw = []\n",
        "sim33 = []\n",
        "sim99 = []"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b1sb_OyfQbA",
        "outputId": "e7a99061-a09e-448a-f43a-f402be8345cf"
      },
      "source": [
        "for i in range(len(word1)):\n",
        "  if word1[i] in dic and word2[i] in dic:\n",
        "    word1Embedding = encoder_result[dic.index(word1[i])]\n",
        "    word2Embedding = encoder_result[dic.index(word2[i])]\n",
        "    sim = 1 - spatial.distance.cosine(word1Embedding,word2Embedding)\n",
        "    sim = np.multiply(sim,10)\n",
        "    w2vsim = \"Missing Word\"\n",
        "    if word1[i] in W2v and word2[i] in W2v:\n",
        "      w2vsim = 1 - spatial.distance.cosine(W2v[word1[i]],W2v[word2[i]])\n",
        "      w2vsim = np.multiply(w2vsim,10)\n",
        "    word11.append(word1[i])\n",
        "    word22.append(word2[i])\n",
        "    sim33.append(sim)    \n",
        "    sim99.append(similarity[i])\n",
        "    simw.append(w2vsim)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUBYK-5UfY2d",
        "outputId": "d58220f5-ea50-4234-b7bf-fad536d4bf1f"
      },
      "source": [
        "for i in range(len(word11)):\n",
        "  print(\"word1\",word11[i],\" word2\",word22[i],\" LSHWE Similarity\",sim33[i],\" word2vec\",simw[i] ,\" wordsim Similarity\", sim99[i])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word1 tiger  word2 tiger  LSHWE Similarity 10.0  word2vec Missing Word  wordsim Similarity 10.0\n",
            "word1 smart  word2 stupid  LSHWE Similarity 9.810423254966736  word2vec Missing Word  wordsim Similarity 5.81\n",
            "word1 king  word2 queen  LSHWE Similarity 9.751085638999939  word2vec Missing Word  wordsim Similarity 8.58\n",
            "word1 fuck  word2 sex  LSHWE Similarity 9.901172518730164  word2vec 9.994485974311829  wordsim Similarity 9.44\n",
            "word1 drink  word2 eat  LSHWE Similarity 9.679554104804993  word2vec Missing Word  wordsim Similarity 6.87\n",
            "word1 money  word2 dollar  LSHWE Similarity 9.682937264442444  word2vec Missing Word  wordsim Similarity 8.42\n",
            "word1 tiger  word2 animal  LSHWE Similarity 9.432339072227478  word2vec Missing Word  wordsim Similarity 7.0\n",
            "word1 planet  word2 sun  LSHWE Similarity 9.774585962295532  word2vec Missing Word  wordsim Similarity 8.02\n",
            "word1 skin  word2 eye  LSHWE Similarity 9.807303547859192  word2vec Missing Word  wordsim Similarity 6.22\n",
            "word1 life  word2 death  LSHWE Similarity 9.885233640670776  word2vec 9.99039113521576  wordsim Similarity 7.88\n",
            "word1 type  word2 kind  LSHWE Similarity 9.937545657157898  word2vec 9.945635199546814  wordsim Similarity 8.97\n",
            "word1 street  word2 place  LSHWE Similarity 9.782588481903076  word2vec Missing Word  wordsim Similarity 6.44\n",
            "word1 man  word2 woman  LSHWE Similarity 9.936739206314087  word2vec 9.997939467430115  wordsim Similarity 8.3\n",
            "word1 glass  word2 metal  LSHWE Similarity 9.778464436531067  word2vec Missing Word  wordsim Similarity 5.56\n",
            "word1 street  word2 children  LSHWE Similarity 9.767671823501587  word2vec Missing Word  wordsim Similarity 4.94\n",
            "word1 start  word2 year  LSHWE Similarity 9.830408692359924  word2vec 9.986838102340698  wordsim Similarity 4.06\n",
            "word1 focus  word2 life  LSHWE Similarity 9.744225144386292  word2vec Missing Word  wordsim Similarity 4.06\n",
            "word1 experience  word2 music  LSHWE Similarity 9.92057204246521  word2vec 9.918050169944763  wordsim Similarity 3.47\n",
            "word1 five  word2 month  LSHWE Similarity 9.760293960571289  word2vec Missing Word  wordsim Similarity 3.38\n",
            "word1 drink  word2 car  LSHWE Similarity 9.69399094581604  word2vec Missing Word  wordsim Similarity 3.04\n",
            "word1 media  word2 gain  LSHWE Similarity 9.601318836212158  word2vec Missing Word  wordsim Similarity 2.88\n",
            "word1 drink  word2 mother  LSHWE Similarity 9.867086410522461  word2vec Missing Word  wordsim Similarity 2.65\n",
            "word1 direction  word2 combination  LSHWE Similarity 9.47458803653717  word2vec Missing Word  wordsim Similarity 2.25\n",
            "word1 holy  word2 sex  LSHWE Similarity 9.708404541015625  word2vec Missing Word  wordsim Similarity 1.62\n",
            "word1 drink  word2 ear  LSHWE Similarity 9.79150116443634  word2vec Missing Word  wordsim Similarity 1.31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc6hjHTWyY2B"
      },
      "source": [
        "Get embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXACX7P7bg3x"
      },
      "source": [
        "embedding_matrix = {}\n",
        "for i in range(len(dic)):\n",
        "  embedding_matrix[dic[i]] = encoder_result[i]\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPdTnaq7ybbD"
      },
      "source": [
        "Add special token - empty and unknown"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C_8NS6PnWEC",
        "outputId": "31b82f8d-baaa-4556-9409-278fb40ef5fa"
      },
      "source": [
        "Unk = \"UNK\"\n",
        "uv = np.zeros(25)\n",
        "embedding_matrix[Unk] = uv\n",
        "dic.append(Unk)\n",
        "empty = \"EMP\"\n",
        "ev = np.zeros(25)\n",
        "embedding_matrix[empty] = ev\n",
        "dic.append(empty)\n",
        "print(len(embedding_matrix))\n",
        "print(type(embedding_matrix))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3654\n",
            "<class 'dict'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxa7cnuAB9F1"
      },
      "source": [
        "Process embedding informaiton to get sum and average vectors for each sentence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYq7Xl5-qgNb"
      },
      "source": [
        "maxl = 12\n",
        "def TTI(sentence):\n",
        "  result = []\n",
        "  for i in sentence:\n",
        "    if i in dic:\n",
        "      result.append(dic.index(i))\n",
        "    else:\n",
        "      result.append(dic.index(\"UNK\"))\n",
        "  while len(result) < 12:\n",
        "    result.append(dic.index(\"EMP\"))\n",
        "  return result[:12]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3Y4JCmt5QPK"
      },
      "source": [
        "def lsh_avg(sentence):\n",
        "  result = np.zeros(200)\n",
        "  for i in sentence:\n",
        "    if i in dic:\n",
        "      result += encoder_result[dic.index(i)]\n",
        "  return result/len(sentence)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV39DwjzKufp"
      },
      "source": [
        "def lsh_sum(sentence):\n",
        "  result = np.zeros(200)\n",
        "  for i in sentence:\n",
        "    if i in dic:\n",
        "      result += encoder_result[dic.index(i)]\n",
        "  return result"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikA3-xyNKTCk"
      },
      "source": [
        "def w2v_avg(sentence):\n",
        "  result = np.zeros(100)\n",
        "  for i in sentence:\n",
        "    if i in W2v:\n",
        "      result += W2v[i]\n",
        "  return result/len(sentence)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9lg3WdtKwNC"
      },
      "source": [
        "def w2v_sum(sentence):\n",
        "  result = np.zeros(100)\n",
        "  for i in sentence:\n",
        "    if i in W2v:\n",
        "      result += W2v[i]\n",
        "  return result"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8bEBZB5wmka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa7211f3-1172-48b8-e00d-8601316f260b"
      },
      "source": [
        "Lav = []\n",
        "Lsu = []\n",
        "Wav = []\n",
        "Wsu = []\n",
        "for i in range(len(comments)):\n",
        "    s = comments[i]\n",
        "    Lav.append(lsh_avg(s))\n",
        "    Lsu.append(lsh_sum(s))\n",
        "    Wav.append(w2v_avg(s))\n",
        "    Wsu.append(w2v_sum(s))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE1CNHHuCeLB"
      },
      "source": [
        "Lav, Lsu, Wav,Wsu,la = sk.utils.shuffle(Lav, Lsu, Wav,Wsu,target,random_state=42)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YJf_Ff58Swp"
      },
      "source": [
        "with open('lshwe_avg.npy', 'wb') as f:\n",
        "  np.save(f,Lav)\n",
        "with open('lshwe_sum.npy', 'wb') as f:\n",
        "  np.save(f,Lsu)\n",
        "with open('w2v_avg.npy', 'wb') as f:\n",
        "  np.save(f,Wav)\n",
        "with open('w2v_sum.npy', 'wb') as f:\n",
        "  np.save(f,Wsu)\n",
        "with open('label.npy', 'wb') as f:\n",
        "  np.save(f,la)"
      ],
      "execution_count": 26,
      "outputs": []
    }
  ]
}