{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semantic.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZtyh1PFyKQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12340f18-fe31-47c4-ebc2-6256a50e0e4d"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5EEFxg_zwN2"
      },
      "source": [
        "# install and import nltk package \n",
        "!sudo pip install -U nltk\n",
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4anLmBmFbACA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8620ec2c-bf0a-4fd2-8c84-2c14ec5568b0"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "import keras\n",
        "import numpy as np\n",
        "import sklearn as sk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW5y8Fm5d08p"
      },
      "source": [
        "data = pd.read_csv(\"Ethos_Dataset_Binary.csv\",delimiter=\";\")\n",
        "label = data[\"isHate\"]\n",
        "comments = data[\"comment\"]\n",
        "target = label.to_numpy()\n",
        "for i in range(len(target)):\n",
        "  if target[i] >= .5:\n",
        "    target[i] = 1\n",
        "  else:\n",
        "    target[i] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlGp5Mt9z9dB"
      },
      "source": [
        "Preprocessing dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fptu2XiUgfLV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e0eba96-3fbb-4824-f4ab-bd4b88819bf6"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "import string\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "for i in range(len(comments)):\n",
        "  comments[i] = word_tokenize(comments[i])\n",
        "  comments[i]=[w.lower() for w in comments[i]]\n",
        "  stripped = [w.translate(table) for w in comments[i]]\n",
        "  comments[i] = [w for w in stripped if w.isalpha()]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj-AcotFeGWH"
      },
      "source": [
        "def dict(x):\n",
        "    dic = []\n",
        "    counts = []\n",
        "    for item in x:\n",
        "        for word in item:\n",
        "            if word in dic:\n",
        "                index = dic.index(word)\n",
        "                counts[index] += 1\n",
        "            else:\n",
        "                dic.append(word)\n",
        "                counts.append(1)\n",
        "    return dic, counts\n",
        "\n",
        "dic, counts = dict(comments)\n",
        "wordcount = len(dic)\n",
        "window_size = 10\n",
        "rarewords_threshold = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGWAucVlxVR3"
      },
      "source": [
        "build co-occurrence matrix "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZNkXFrVZ7Pg"
      },
      "source": [
        "cooc_matr = np.zeros([wordcount, wordcount])\n",
        "for item in comments:\n",
        "    for i in range(len(item)):\n",
        "        cenword = item[i]\n",
        "        cen_id = dic.index(cenword)\n",
        "        for j in range(i - window_size, i + window_size + 1):\n",
        "            if j in range(len(item)):\n",
        "                val = window_size + 1 - abs(i - j)\n",
        "                coword = item[j]\n",
        "                co_id = dic.index(coword)\n",
        "                cooc_matr[cen_id][co_id] += val\n",
        "\n",
        "for i in range(wordcount):\n",
        "    cooc_matr[i] = cooc_matr[i] / cooc_matr[i][i] * 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwrcnnbgxf4P"
      },
      "source": [
        "Getting rarewords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JCNSqWhaWKF"
      },
      "source": [
        "rarewords = []\n",
        "for i in range(wordcount):\n",
        "    if counts[i] < rarewords_threshold:\n",
        "        rarewords.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia0f1gOw9rqn"
      },
      "source": [
        "#Local sensitivity hashing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_Dn74bExj_w"
      },
      "source": [
        "Use local sensitivity hashing on co-occurrence matrix "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbRklUVjn__6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45546581-ad37-4c39-dd60-190e307365c2"
      },
      "source": [
        "pip install lshashpy3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lshashpy3 in /usr/local/lib/python3.7/dist-packages (0.0.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLHVK-ZtrHTS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeed0dd1-fcf5-49ca-94bd-c1286254286b"
      },
      "source": [
        "from lshashpy3 import LSHash\n",
        "hashsize = 200\n",
        "friends = 5\n",
        "#lsh = LSHash(window_size, wordcount)\n",
        "lsh = LSHash(hashsize, wordcount)\n",
        "for i in range(len(cooc_matr)):\n",
        "    lsh.index(cooc_matr[i])\n",
        "\n",
        "friendslist = []\n",
        "sim_matr = []\n",
        "for i in range(len(rarewords)):\n",
        "    result = lsh.query(cooc_matr[rarewords[i]], num_results=friends, distance_func='euclidean')\n",
        "    length = len(result)\n",
        "    fri = []\n",
        "    sim = []\n",
        "    for j in range(length):\n",
        "        fri.append(str(np.array(result[j][0])))\n",
        "        sim.append(float(result[j][1]))\n",
        "    if length < friends:\n",
        "        count = friends - length\n",
        "        for i in range(count):\n",
        "            fri.append(str(8))\n",
        "            sim.append(-1)\n",
        "    friendslist.append(fri)\n",
        "    sim_matr.append(sim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMbDiI1KxvMT"
      },
      "source": [
        "Start trainning a embedding to approximate LSH "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM5wA2z9xpvo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4686b179-6c69-4402-9ddb-c08cf1ba5e70"
      },
      "source": [
        "dim = 200\n",
        "learning_rate = 0.01\n",
        "training_epoch = 50\n",
        "X = tf.placeholder(\"float\", [wordcount, wordcount])\n",
        "sim_true = tf.placeholder(\"float\", [len(rarewords), friends])\n",
        "sim_pred = tf.placeholder(\"float\", [len(rarewords), friends])\n",
        "n_hidden_1 = 200\n",
        "n_hidden_2 = dim\n",
        "\n",
        "weights = {\n",
        "    'encoder_h1': tf.Variable(tf.random_normal([wordcount, n_hidden_1])),\n",
        "    'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
        "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
        "    'decoder_h2': tf.Variable(tf.random_normal([n_hidden_1, wordcount])),\n",
        "}\n",
        "biases = {\n",
        "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
        "    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "    'decoder_b2': tf.Variable(tf.random_normal([wordcount])),\n",
        "}\n",
        "\n",
        "def encoder(x):\n",
        "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
        "                                   biases['encoder_b1']))\n",
        "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n",
        "                                   biases['encoder_b2']))\n",
        "    return layer_2\n",
        "\n",
        "def decoder(x):\n",
        "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
        "                                   biases['decoder_b1']))\n",
        "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
        "                                   biases['decoder_b2']))\n",
        "    return layer_2\n",
        "\n",
        "cooc_true = X\n",
        "encoder_op = encoder(X)\n",
        "cooc_pred = decoder(encoder_op)\n",
        "cost = tf.reduce_mean(tf.pow(cooc_true - cooc_pred, 2))\n",
        "cost_min = cost + tf.reduce_mean(tf.pow(sim_true - sim_pred, 2))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost_min)\n",
        "\n",
        "if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
        "    init = tf.initialize_all_variables()\n",
        "else:\n",
        "    init = tf.global_variables_initializer()\n",
        "with tf.Session() as sess:\n",
        "    sess.graph.finalize()\n",
        "    sess.run(init)\n",
        "    for epoch in range(training_epoch):\n",
        "        word_list = sess.run(encoder_op, feed_dict={X: cooc_matr})\n",
        "\n",
        "        lsh = LSHash(hashsize, dim)\n",
        "        for item in word_list:\n",
        "            lsh.index(item)\n",
        "\n",
        "        cos = []\n",
        "        for i in range(len(rarewords)):\n",
        "            result = lsh.query(word_list[rarewords[i]], num_results=friends, distance_func='euclidean')\n",
        "            cosin = -1 * np.ones(friends)\n",
        "            for item in result:\n",
        "                id = str(np.array(item[0]))\n",
        "                for j in range(friends):\n",
        "                    if id == friendslist[i][j]:\n",
        "                        cosin[j] = float(item[1])\n",
        "            cos.append(cosin)\n",
        "\n",
        "        cost_fin, _ = sess.run([cost_min, optimizer], feed_dict={sim_pred: cos, sim_true: sim_matr, X: cooc_matr})\n",
        "        print(\"epoch:\",epoch, \",cost=\", \"{:.9f}\".format(cost_fin))\n",
        "    print(\"Optimization Finished!\")\n",
        "    encoder_result = sess.run(encoder_op, feed_dict={X: cooc_matr})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 ,cost= 0.659681857\n",
            "epoch: 1 ,cost= 0.610038936\n",
            "epoch: 2 ,cost= 0.566093504\n",
            "epoch: 3 ,cost= 0.528135777\n",
            "epoch: 4 ,cost= 0.494091809\n",
            "epoch: 5 ,cost= 0.463117778\n",
            "epoch: 6 ,cost= 0.434807777\n",
            "epoch: 7 ,cost= 0.409109950\n",
            "epoch: 8 ,cost= 0.386078835\n",
            "epoch: 9 ,cost= 0.366051644\n",
            "epoch: 10 ,cost= 0.349022150\n",
            "epoch: 11 ,cost= 0.334097505\n",
            "epoch: 12 ,cost= 0.320876658\n",
            "epoch: 13 ,cost= 0.309738696\n",
            "epoch: 14 ,cost= 0.300465912\n",
            "epoch: 15 ,cost= 0.292656451\n",
            "epoch: 16 ,cost= 0.286193073\n",
            "epoch: 17 ,cost= 0.280950785\n",
            "epoch: 18 ,cost= 0.276715040\n",
            "epoch: 19 ,cost= 0.273261905\n",
            "epoch: 20 ,cost= 0.270221919\n",
            "epoch: 21 ,cost= 0.267396957\n",
            "epoch: 22 ,cost= 0.264886707\n",
            "epoch: 23 ,cost= 0.262795031\n",
            "epoch: 24 ,cost= 0.261137486\n",
            "epoch: 25 ,cost= 0.259684950\n",
            "epoch: 26 ,cost= 0.258180976\n",
            "epoch: 27 ,cost= 0.256685942\n",
            "epoch: 28 ,cost= 0.255419850\n",
            "epoch: 29 ,cost= 0.254364043\n",
            "epoch: 30 ,cost= 0.253394842\n",
            "epoch: 31 ,cost= 0.252517700\n",
            "epoch: 32 ,cost= 0.251750797\n",
            "epoch: 33 ,cost= 0.251137078\n",
            "epoch: 34 ,cost= 0.250672370\n",
            "epoch: 35 ,cost= 0.250211835\n",
            "epoch: 36 ,cost= 0.249699220\n",
            "epoch: 37 ,cost= 0.249189660\n",
            "epoch: 38 ,cost= 0.248702794\n",
            "epoch: 39 ,cost= 0.248233631\n",
            "epoch: 40 ,cost= 0.247735888\n",
            "epoch: 41 ,cost= 0.247230530\n",
            "epoch: 42 ,cost= 0.246827245\n",
            "epoch: 43 ,cost= 0.246527344\n",
            "epoch: 44 ,cost= 0.246204048\n",
            "epoch: 45 ,cost= 0.245779648\n",
            "epoch: 46 ,cost= 0.245207459\n",
            "epoch: 47 ,cost= 0.244462997\n",
            "epoch: 48 ,cost= 0.243699864\n",
            "epoch: 49 ,cost= 0.243150681\n",
            "Optimization Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhpsh6IWyA3z"
      },
      "source": [
        "load Word2Vec "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNEYfOFHYPyX"
      },
      "source": [
        "W2v = Word2Vec(comments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o8P4gdUyEm8"
      },
      "source": [
        "Begin word similarity task, using wordsim355 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAR5awEueume"
      },
      "source": [
        "from scipy import spatial\n",
        "wordsim= pd.read_csv(\"wordsim.txt\",delimiter = \"\\t\")\n",
        "word1 = wordsim[\"tiger\"]\n",
        "word2 = wordsim[\"cat\"]\n",
        "similarity = wordsim[\"7.35\"]\n",
        "word11 = []\n",
        "word22 = []\n",
        "w2v1 = []\n",
        "w2v2 = []\n",
        "simw = []\n",
        "sim33 = []\n",
        "sim99 = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b1sb_OyfQbA",
        "outputId": "32e86c4a-3718-490c-e5c2-06fa6eb5d1d0"
      },
      "source": [
        "for i in range(len(word1)):\n",
        "  if word1[i] in dic and word2[i] in dic:\n",
        "    word1Embedding = encoder_result[dic.index(word1[i])]\n",
        "    word2Embedding = encoder_result[dic.index(word2[i])]\n",
        "    sim = 1 - spatial.distance.cosine(word1Embedding,word2Embedding)\n",
        "    sim = np.multiply(sim,10)\n",
        "    w2vsim = \"Missing Word\"\n",
        "    if word1[i] in W2v and word2[i] in W2v:\n",
        "      w2vsim = 1 - spatial.distance.cosine(W2v[word1[i]],W2v[word2[i]])\n",
        "      w2vsim = np.multiply(w2vsim,10)\n",
        "    word11.append(word1[i])\n",
        "    word22.append(word2[i])\n",
        "    sim33.append(sim)    \n",
        "    sim99.append(similarity[i])\n",
        "    simw.append(w2vsim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUBYK-5UfY2d",
        "outputId": "c1e051a3-72c8-4491-fabb-c0d1191aa381"
      },
      "source": [
        "for i in range(len(word11)):\n",
        "  print(\"word1\",word11[i],\" word2\",word22[i],\" LSHWE Similarity\",sim33[i],\" word2vec\",simw[i] ,\" wordsim Similarity\", sim99[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word1 tiger  word2 tiger  LSHWE Similarity 10.0  word2vec Missing Word  wordsim Similarity 10.0\n",
            "word1 smart  word2 stupid  LSHWE Similarity 9.8575758934021  word2vec Missing Word  wordsim Similarity 5.81\n",
            "word1 king  word2 queen  LSHWE Similarity 9.729621410369873  word2vec Missing Word  wordsim Similarity 8.58\n",
            "word1 fuck  word2 sex  LSHWE Similarity 9.931595921516418  word2vec 9.996511340141296  wordsim Similarity 9.44\n",
            "word1 drink  word2 eat  LSHWE Similarity 9.825449585914612  word2vec Missing Word  wordsim Similarity 6.87\n",
            "word1 money  word2 dollar  LSHWE Similarity 9.713850021362305  word2vec Missing Word  wordsim Similarity 8.42\n",
            "word1 tiger  word2 animal  LSHWE Similarity 9.653976559638977  word2vec Missing Word  wordsim Similarity 7.0\n",
            "word1 planet  word2 sun  LSHWE Similarity 9.836339950561523  word2vec Missing Word  wordsim Similarity 8.02\n",
            "word1 skin  word2 eye  LSHWE Similarity 9.720719456672668  word2vec Missing Word  wordsim Similarity 6.22\n",
            "word1 life  word2 death  LSHWE Similarity 9.766581058502197  word2vec 9.989002346992493  wordsim Similarity 7.88\n",
            "word1 type  word2 kind  LSHWE Similarity 9.91054117679596  word2vec 9.948816299438477  wordsim Similarity 8.97\n",
            "word1 street  word2 place  LSHWE Similarity 9.682806730270386  word2vec Missing Word  wordsim Similarity 6.44\n",
            "word1 man  word2 woman  LSHWE Similarity 9.89560604095459  word2vec 9.997804164886475  wordsim Similarity 8.3\n",
            "word1 glass  word2 metal  LSHWE Similarity 9.721524715423584  word2vec Missing Word  wordsim Similarity 5.56\n",
            "word1 street  word2 children  LSHWE Similarity 9.686262607574463  word2vec Missing Word  wordsim Similarity 4.94\n",
            "word1 start  word2 year  LSHWE Similarity 9.794750809669495  word2vec 9.987203478813171  wordsim Similarity 4.06\n",
            "word1 focus  word2 life  LSHWE Similarity 9.789291024208069  word2vec Missing Word  wordsim Similarity 4.06\n",
            "word1 experience  word2 music  LSHWE Similarity 9.825776219367981  word2vec 9.925318360328674  wordsim Similarity 3.47\n",
            "word1 five  word2 month  LSHWE Similarity 9.63102638721466  word2vec Missing Word  wordsim Similarity 3.38\n",
            "word1 drink  word2 car  LSHWE Similarity 9.640308618545532  word2vec Missing Word  wordsim Similarity 3.04\n",
            "word1 media  word2 gain  LSHWE Similarity 9.63618278503418  word2vec Missing Word  wordsim Similarity 2.88\n",
            "word1 drink  word2 mother  LSHWE Similarity 9.78573203086853  word2vec Missing Word  wordsim Similarity 2.65\n",
            "word1 direction  word2 combination  LSHWE Similarity 9.705985188484192  word2vec Missing Word  wordsim Similarity 2.25\n",
            "word1 holy  word2 sex  LSHWE Similarity 9.852678775787354  word2vec Missing Word  wordsim Similarity 1.62\n",
            "word1 drink  word2 ear  LSHWE Similarity 9.774275422096252  word2vec Missing Word  wordsim Similarity 1.31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc6hjHTWyY2B"
      },
      "source": [
        "Get embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXACX7P7bg3x"
      },
      "source": [
        "embedding_matrix = {}\n",
        "for i in range(len(dic)):\n",
        "  embedding_matrix[dic[i]] = encoder_result[i]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPdTnaq7ybbD"
      },
      "source": [
        "Add special token - empty and unknown"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C_8NS6PnWEC",
        "outputId": "eb32e30c-59ab-4ad1-8afd-b4d09b9b37e6"
      },
      "source": [
        "Unk = \"UNK\"\n",
        "uv = np.zeros(25)\n",
        "embedding_matrix[Unk] = uv\n",
        "dic.append(Unk)\n",
        "empty = \"EMP\"\n",
        "ev = np.zeros(25)\n",
        "embedding_matrix[empty] = ev\n",
        "dic.append(empty)\n",
        "print(len(embedding_matrix))\n",
        "print(type(embedding_matrix))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3654\n",
            "<class 'dict'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxa7cnuAB9F1"
      },
      "source": [
        "Process embedding informaiton to get sum and average vectors for each sentence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYq7Xl5-qgNb"
      },
      "source": [
        "maxl = 12\n",
        "def TTI(sentence):\n",
        "  result = []\n",
        "  for i in sentence:\n",
        "    if i in dic:\n",
        "      result.append(dic.index(i))\n",
        "    else:\n",
        "      result.append(dic.index(\"UNK\"))\n",
        "  while len(result) < 12:\n",
        "    result.append(dic.index(\"EMP\"))\n",
        "  return result[:12]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3Y4JCmt5QPK"
      },
      "source": [
        "def lsh_avg(sentence):\n",
        "  result = np.zeros(200)\n",
        "  for i in sentence:\n",
        "    if i in dic:\n",
        "      result += encoder_result[dic.index(i)]\n",
        "  return result/len(sentence)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV39DwjzKufp"
      },
      "source": [
        "def lsh_sum(sentence):\n",
        "  result = np.zeros(200)\n",
        "  for i in sentence:\n",
        "    if i in dic:\n",
        "      result += encoder_result[dic.index(i)]\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikA3-xyNKTCk"
      },
      "source": [
        "def w2v_avg(sentence):\n",
        "  result = np.zeros(100)\n",
        "  for i in sentence:\n",
        "    if i in W2v:\n",
        "      result += W2v[i]\n",
        "  return result/len(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9lg3WdtKwNC"
      },
      "source": [
        "def w2v_sum(sentence):\n",
        "  result = np.zeros(100)\n",
        "  for i in sentence:\n",
        "    if i in W2v:\n",
        "      result += W2v[i]\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8bEBZB5wmka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e8c7719-9edb-41a2-e27d-c70c45148dbf"
      },
      "source": [
        "Lav = []\n",
        "Lsu = []\n",
        "Wav = []\n",
        "Wsu = []\n",
        "for i in range(len(comments)):\n",
        "    s = comments[i]\n",
        "    Lav.append(lsh_avg(s))\n",
        "    Lsu.append(lsh_sum(s))\n",
        "    Wav.append(w2v_avg(s))\n",
        "    Wsu.append(w2v_sum(s))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE1CNHHuCeLB"
      },
      "source": [
        "Lav, Lsu, Wav,Wsu,la = sk.utils.shuffle(Lav, Lsu, Wav,Wsu,target,random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YJf_Ff58Swp"
      },
      "source": [
        "with open('lshwe_avg.npy', 'wb') as f:\n",
        "  np.save(f,Lav)\n",
        "with open('lshwe_sum.npy', 'wb') as f:\n",
        "  np.save(f,Lsu)\n",
        "with open('w2v_avg.npy', 'wb') as f:\n",
        "  np.save(f,Wav)\n",
        "with open('w2v_sum.npy', 'wb') as f:\n",
        "  np.save(f,Wsu)\n",
        "with open('label.npy', 'wb') as f:\n",
        "  np.save(f,la)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
